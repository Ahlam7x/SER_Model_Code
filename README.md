Here's an organized and professional version of your README section, avoiding repetition and including links to the datasets used:

---

# **Speech Emotion Recognition (SER) Model**

This repository implements a **Speech Emotion Recognition (SER) model** that integrates a **Co-Attention mechanism** to enhance feature fusion across multiple models. The architecture is inspired by the original [transformer-cnn-emotion-recognition](https://github.com/IliaZenkov/transformer-cnn-emotion-recognition) repository by Ilia Zenkov, with significant improvements and extensions.

---

## **üìå Overview**

This repository contains the implementation of a **Speech Emotion Recognition (SER) model** that utilizes advanced deep learning techniques for emotion classification in speech. The key features of this implementation are:

- **Co-Attention Mechanism:** A novel feature fusion technique that combines multiple model outputs for better performance.
- **Multi-Dataset Application:** The model is evaluated on three diverse datasets, namely **ASVP-ESD V1**, **ASVP-ESD V2**, and **ShEmo**.
- **Feature Extraction:** Combines **MFCC** and **Mel spectrograms** to improve emotion recognition accuracy.

üîç **Inspired by:** [transformer-cnn-emotion-recognition](https://github.com/IliaZenkov/transformer-cnn-emotion-recognition) by Ilia Zenkov.

---

## **üìú Acknowledgements**

This work is largely inspired by the [transformer-cnn-emotion-recognition](https://github.com/IliaZenkov/transformer-cnn-emotion-recognition) repository by **Ilia Zenkov**. We have extended the original architecture with the following contributions:

- **Co-Attention Mechanism:** Added a Co-Attention layer to combine features from multiple models, improving the model‚Äôs ability to learn from diverse inputs.
- **Application to Multiple Datasets:** Evaluated the model on three datasets‚Äî**ASVP-ESD V1**, **ASVP-ESD V2**, and **ShEmo**‚Äîto assess its performance across different conditions and speakers.
- **Improved Accuracy:** Enhanced emotion classification by combining multiple feature extraction techniques, including MFCCs and Mel spectrograms.

---

## **üìö Datasets Used**

This model is evaluated on the following datasets:

1. **[ASVP-ESD V1](https://github.com/your-link-to-ASVP-ESD-V1)**
2. **[ASVP-ESD V2](https://github.com/your-link-to-ASVP-ESD-V2)**
3. **[ShEmo](https://github.com/your-link-to-ShEmo)**

These datasets provide a diverse range of speech samples, including multiple emotions, speakers, and languages, which enables thorough evaluation of the model‚Äôs performance.

---

